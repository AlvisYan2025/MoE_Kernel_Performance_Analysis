/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/myenvs/vllm_env_conda/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
INFO 12-06 14:37:04 [__init__.py:241] Automatically detected platform cuda.
WARNING 12-06 14:37:07 [__init__.py:1750] argument '--disable-log-requests' is deprecated and replaced with '--enable-log-requests'. This will be removed in v0.12.0.
[1;36m(APIServer pid=594587)[0;0m INFO 12-06 14:37:07 [api_server.py:1875] vLLM API server version 0.10.1rc2.dev263+g2f13319f4
[1;36m(APIServer pid=594587)[0;0m INFO 12-06 14:37:07 [utils.py:326] non-default args: {'model_tag': 'mistralai/Mixtral-8x7B-v0.1', 'host': '0.0.0.0', 'port': 8001, 'model': 'mistralai/Mixtral-8x7B-v0.1', 'max_model_len': 4096, 'max_num_batched_tokens': 4096, 'max_num_seqs': 16}
[1;36m(APIServer pid=594587)[0;0m INFO 12-06 14:37:16 [__init__.py:742] Resolved architecture: MixtralForCausalLM
[1;36m(APIServer pid=594587)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=594587)[0;0m INFO 12-06 14:37:16 [__init__.py:1786] Using max model len 4096
[1;36m(APIServer pid=594587)[0;0m INFO 12-06 14:37:17 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=4096.
[1;36m(APIServer pid=594587)[0;0m /pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/transformers_utils/tokenizer_group.py:26: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(APIServer pid=594587)[0;0m   self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)
/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/myenvs/vllm_env_conda/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
INFO 12-06 14:37:23 [__init__.py:241] Automatically detected platform cuda.
[1;36m(EngineCore_0 pid=594898)[0;0m INFO 12-06 14:37:25 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_0 pid=594898)[0;0m INFO 12-06 14:37:25 [core.py:74] Initializing a V1 LLM engine (v0.10.1rc2.dev263+g2f13319f4) with config: model='mistralai/Mixtral-8x7B-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mixtral-8x7B-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":32,"local_cache_dir":null}
[1;36m(EngineCore_0 pid=594898)[0;0m INFO 12-06 14:37:28 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_0 pid=594898)[0;0m WARNING 12-06 14:37:28 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_0 pid=594898)[0;0m INFO 12-06 14:37:28 [gpu_model_runner.py:1929] Starting to load model mistralai/Mixtral-8x7B-v0.1...
[1;36m(EngineCore_0 pid=594898)[0;0m INFO 12-06 14:37:28 [gpu_model_runner.py:1961] Loading model from scratch...
[1;36m(EngineCore_0 pid=594898)[0;0m INFO 12-06 14:37:28 [cuda.py:328] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708] EngineCore failed to start.
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708] Traceback (most recent call last):
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/v1/engine/core.py", line 699, in run_engine_core
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/v1/engine/core.py", line 500, in __init__
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/v1/engine/core.py", line 80, in __init__
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/executor/executor_base.py", line 54, in __init__
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]     self._init_executor()
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/executor/uniproc_executor.py", line 49, in _init_executor
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]     self.collective_rpc("load_model")
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/executor/uniproc_executor.py", line 58, in collective_rpc
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]     answer = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/utils/__init__.py", line 3031, in run_method
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]     return func(*args, **kwargs)
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/v1/worker/gpu_worker.py", line 213, in load_model
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/v1/worker/gpu_model_runner.py", line 1962, in load_model
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]     self.model = model_loader.load_model(
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/model_executor/model_loader/base_loader.py", line 44, in load_model
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]     model = initialize_model(vllm_config=vllm_config,
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/model_executor/model_loader/utils.py", line 63, in initialize_model
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/model_executor/models/mixtral.py", line 442, in __init__
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]     self.model = MixtralModel(vllm_config=vllm_config,
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/compilation/decorators.py", line 199, in __init__
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/model_executor/models/mixtral.py", line 278, in __init__
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/model_executor/models/utils.py", line 640, in make_layers
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]     [PPMissingLayer() for _ in range(start_layer)] + [
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]                                                      ^
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/model_executor/models/utils.py", line 641, in <listcomp>
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/model_executor/models/mixtral.py", line 280, in <lambda>
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]     lambda prefix: MixtralDecoderLayer(
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]                    ^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/model_executor/models/mixtral.py", line 217, in __init__
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]     self.block_sparse_moe = MixtralMoE(
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]                             ^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/model_executor/models/mixtral.py", line 89, in __init__
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]     self.experts = FusedMoE(num_experts=num_experts,
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/model_executor/layers/fused_moe/layer.py", line 914, in __init__
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]     self.quant_method.create_weights(layer=self, **moe_quant_params)
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/model_executor/layers/fused_moe/layer.py", line 292, in create_weights
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]     w13_weight = torch.nn.Parameter(torch.empty(
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]                                     ^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/myenvs/vllm_env_conda/lib/python3.11/site-packages/torch/utils/_device.py", line 104, in __torch_function__
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]     return func(*args, **kwargs)
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m ERROR 12-06 14:37:29 [core.py:708] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.75 GiB. GPU 0 has a total capacity of 39.38 GiB of which 690.12 MiB is free. Including non-PyTorch memory, this process has 38.70 GiB memory in use. Of the allocated memory 38.21 GiB is allocated by PyTorch, and 12.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_0 pid=594898)[0;0m Process EngineCore_0:
[1;36m(EngineCore_0 pid=594898)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/myenvs/vllm_env_conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_0 pid=594898)[0;0m     self.run()
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/myenvs/vllm_env_conda/lib/python3.11/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_0 pid=594898)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/v1/engine/core.py", line 712, in run_engine_core
[1;36m(EngineCore_0 pid=594898)[0;0m     raise e
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/v1/engine/core.py", line 699, in run_engine_core
[1;36m(EngineCore_0 pid=594898)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_0 pid=594898)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/v1/engine/core.py", line 500, in __init__
[1;36m(EngineCore_0 pid=594898)[0;0m     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/v1/engine/core.py", line 80, in __init__
[1;36m(EngineCore_0 pid=594898)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_0 pid=594898)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/executor/executor_base.py", line 54, in __init__
[1;36m(EngineCore_0 pid=594898)[0;0m     self._init_executor()
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/executor/uniproc_executor.py", line 49, in _init_executor
[1;36m(EngineCore_0 pid=594898)[0;0m     self.collective_rpc("load_model")
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/executor/uniproc_executor.py", line 58, in collective_rpc
[1;36m(EngineCore_0 pid=594898)[0;0m     answer = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_0 pid=594898)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/utils/__init__.py", line 3031, in run_method
[1;36m(EngineCore_0 pid=594898)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_0 pid=594898)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/v1/worker/gpu_worker.py", line 213, in load_model
[1;36m(EngineCore_0 pid=594898)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/v1/worker/gpu_model_runner.py", line 1962, in load_model
[1;36m(EngineCore_0 pid=594898)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_0 pid=594898)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/model_executor/model_loader/base_loader.py", line 44, in load_model
[1;36m(EngineCore_0 pid=594898)[0;0m     model = initialize_model(vllm_config=vllm_config,
[1;36m(EngineCore_0 pid=594898)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/model_executor/model_loader/utils.py", line 63, in initialize_model
[1;36m(EngineCore_0 pid=594898)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_0 pid=594898)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/model_executor/models/mixtral.py", line 442, in __init__
[1;36m(EngineCore_0 pid=594898)[0;0m     self.model = MixtralModel(vllm_config=vllm_config,
[1;36m(EngineCore_0 pid=594898)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/compilation/decorators.py", line 199, in __init__
[1;36m(EngineCore_0 pid=594898)[0;0m     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/model_executor/models/mixtral.py", line 278, in __init__
[1;36m(EngineCore_0 pid=594898)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_0 pid=594898)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/model_executor/models/utils.py", line 640, in make_layers
[1;36m(EngineCore_0 pid=594898)[0;0m     [PPMissingLayer() for _ in range(start_layer)] + [
[1;36m(EngineCore_0 pid=594898)[0;0m                                                      ^
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/model_executor/models/utils.py", line 641, in <listcomp>
[1;36m(EngineCore_0 pid=594898)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_0 pid=594898)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/model_executor/models/mixtral.py", line 280, in <lambda>
[1;36m(EngineCore_0 pid=594898)[0;0m     lambda prefix: MixtralDecoderLayer(
[1;36m(EngineCore_0 pid=594898)[0;0m                    ^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/model_executor/models/mixtral.py", line 217, in __init__
[1;36m(EngineCore_0 pid=594898)[0;0m     self.block_sparse_moe = MixtralMoE(
[1;36m(EngineCore_0 pid=594898)[0;0m                             ^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/model_executor/models/mixtral.py", line 89, in __init__
[1;36m(EngineCore_0 pid=594898)[0;0m     self.experts = FusedMoE(num_experts=num_experts,
[1;36m(EngineCore_0 pid=594898)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/model_executor/layers/fused_moe/layer.py", line 914, in __init__
[1;36m(EngineCore_0 pid=594898)[0;0m     self.quant_method.create_weights(layer=self, **moe_quant_params)
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/model_executor/layers/fused_moe/layer.py", line 292, in create_weights
[1;36m(EngineCore_0 pid=594898)[0;0m     w13_weight = torch.nn.Parameter(torch.empty(
[1;36m(EngineCore_0 pid=594898)[0;0m                                     ^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/myenvs/vllm_env_conda/lib/python3.11/site-packages/torch/utils/_device.py", line 104, in __torch_function__
[1;36m(EngineCore_0 pid=594898)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_0 pid=594898)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=594898)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.75 GiB. GPU 0 has a total capacity of 39.38 GiB of which 690.12 MiB is free. Including non-PyTorch memory, this process has 38.70 GiB memory in use. Of the allocated memory 38.21 GiB is allocated by PyTorch, and 12.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1206 14:37:29.342105415 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[1;36m(APIServer pid=594587)[0;0m Traceback (most recent call last):
[1;36m(APIServer pid=594587)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/myenvs/vllm_env_conda/bin/vllm", line 7, in <module>
[1;36m(APIServer pid=594587)[0;0m     sys.exit(main())
[1;36m(APIServer pid=594587)[0;0m              ^^^^^^
[1;36m(APIServer pid=594587)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/entrypoints/cli/main.py", line 54, in main
[1;36m(APIServer pid=594587)[0;0m     args.dispatch_function(args)
[1;36m(APIServer pid=594587)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/entrypoints/cli/serve.py", line 50, in cmd
[1;36m(APIServer pid=594587)[0;0m     uvloop.run(run_server(args))
[1;36m(APIServer pid=594587)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/myenvs/vllm_env_conda/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[1;36m(APIServer pid=594587)[0;0m     return runner.run(wrapper())
[1;36m(APIServer pid=594587)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=594587)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/myenvs/vllm_env_conda/lib/python3.11/asyncio/runners.py", line 118, in run
[1;36m(APIServer pid=594587)[0;0m     return self._loop.run_until_complete(task)
[1;36m(APIServer pid=594587)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=594587)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[1;36m(APIServer pid=594587)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/myenvs/vllm_env_conda/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[1;36m(APIServer pid=594587)[0;0m     return await main
[1;36m(APIServer pid=594587)[0;0m            ^^^^^^^^^^
[1;36m(APIServer pid=594587)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/entrypoints/openai/api_server.py", line 1920, in run_server
[1;36m(APIServer pid=594587)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[1;36m(APIServer pid=594587)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/entrypoints/openai/api_server.py", line 1940, in run_server_worker
[1;36m(APIServer pid=594587)[0;0m     async with build_async_engine_client(
[1;36m(APIServer pid=594587)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/myenvs/vllm_env_conda/lib/python3.11/contextlib.py", line 210, in __aenter__
[1;36m(APIServer pid=594587)[0;0m     return await anext(self.gen)
[1;36m(APIServer pid=594587)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=594587)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/entrypoints/openai/api_server.py", line 178, in build_async_engine_client
[1;36m(APIServer pid=594587)[0;0m     async with build_async_engine_client_from_engine_args(
[1;36m(APIServer pid=594587)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/myenvs/vllm_env_conda/lib/python3.11/contextlib.py", line 210, in __aenter__
[1;36m(APIServer pid=594587)[0;0m     return await anext(self.gen)
[1;36m(APIServer pid=594587)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=594587)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/entrypoints/openai/api_server.py", line 220, in build_async_engine_client_from_engine_args
[1;36m(APIServer pid=594587)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[1;36m(APIServer pid=594587)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=594587)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/utils/__init__.py", line 1581, in inner
[1;36m(APIServer pid=594587)[0;0m     return fn(*args, **kwargs)
[1;36m(APIServer pid=594587)[0;0m            ^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=594587)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/v1/engine/async_llm.py", line 197, in from_vllm_config
[1;36m(APIServer pid=594587)[0;0m     return cls(
[1;36m(APIServer pid=594587)[0;0m            ^^^^
[1;36m(APIServer pid=594587)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/v1/engine/async_llm.py", line 123, in __init__
[1;36m(APIServer pid=594587)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[1;36m(APIServer pid=594587)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=594587)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/v1/engine/core_client.py", line 102, in make_async_mp_client
[1;36m(APIServer pid=594587)[0;0m     return AsyncMPClient(*client_args)
[1;36m(APIServer pid=594587)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=594587)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/v1/engine/core_client.py", line 767, in __init__
[1;36m(APIServer pid=594587)[0;0m     super().__init__(
[1;36m(APIServer pid=594587)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/v1/engine/core_client.py", line 446, in __init__
[1;36m(APIServer pid=594587)[0;0m     with launch_core_engines(vllm_config, executor_class,
[1;36m(APIServer pid=594587)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/myenvs/vllm_env_conda/lib/python3.11/contextlib.py", line 144, in __exit__
[1;36m(APIServer pid=594587)[0;0m     next(self.gen)
[1;36m(APIServer pid=594587)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/v1/engine/utils.py", line 706, in launch_core_engines
[1;36m(APIServer pid=594587)[0;0m     wait_for_engine_startup(
[1;36m(APIServer pid=594587)[0;0m   File "/pscratch/sd/y/yz3526/MoE_Kernel_Performance_Analysis/vllm/vllm/v1/engine/utils.py", line 759, in wait_for_engine_startup
[1;36m(APIServer pid=594587)[0;0m     raise RuntimeError("Engine core initialization failed. "
[1;36m(APIServer pid=594587)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
